* Paper notes : Estimating the environmental impacts of AI inference deployments
*** Djoser SIMEU M2 MOSIG DSAI / DATAMOVE
** 1° paper : Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning (https://www.sciencedirect.com/science/article/pii/S2210537923000124?ref=pdf_download&fr=RR-2&rr=90c1945e6bdfbb5c)
*** Authors : Radosvet Desislavov, Fernando Martínez-Plumed, José Hernández-Orallo
*** Keywords :
inference costs, energy consumption, computer vision, natural language processing, carbon footprint, FLOPs,
*** Citations:
+ "At first look it seems that training cost is higher. However, for deployed systems, inference costs exceed training costs, because of the multiplicative factor of using the system many times."
+ "We focus our analysis on inference FLOPs (Floating Point Operations) required to process one input item (image or text fragment)."
*** SOTA :
+ "AI efficiency" : D. Hernandez, T.B. Brown, Measuring the algorithmic efficiency of neural networks, 2020, arXiv:2005.04305.
" This study shows that 44 times less compute
was required in 2020 to train a network with the same performancevAlexNet achieved seven years before."
+ "D. Amodei, D. Hernandez, Ai and compute, 2018, https://openai.com/blog/aiand-compute/."
 "As a result, it has been estimated that AI models have doubled the computational power they use every 3.4 months since 2012" + "For instance, new algorithms and architectures such as EfficientNet [26] and EfficientNetV2 [27] have aimed at this reduction in compute"
*** Observations :
+ The number of parameters and FLOPs: "Note that recent transformer models [45] do not follow the growth relation presented above. However, the correlation between the number of parameters and FLOPs for CNNs is 0.772 and the correlation for transformers is 0.994 (Fig. 1)."
***** TODO :: How do they compute FLOPs for transformer models
+ "From the FLOPS and power consumption we calculate the efficiency, dividing FLOPS by Watts."
 #+begin_center
  [[file:images/Energy_Efficiency_comp.png]]
  #+end_center
+ "For NLP, the trends are similar, but the best models are growing much faster, as we see in Fig. 14, while the regular models may even decrease. "
  #+begin_center
[[file:images/NLP_Task_Joules.png]]
[[file:images/NLP_Task_GLUE.png]]
#+end_center
** 2° paper : Estimating the environmental impact of Generative-AI services using an LCA-based methodology (https://www.sciencedirect.com/science/article/pii/S2212827124001173)
*** Authors : Adrien Berthelot, Eddy Caron, Mathilde Jay, Laurent Lefevre
*** Keywords :
 LCA-based methodology, environmental impact of generative AI services, Stable Diffusion
*** Citations :
+ "In the context of environmental challenges and considering the footprint of the digital sector, which in the EU already accounts for 9.3% of electricity consumption and over 4% of greenhouse gas emissions, many studies have addressed the question of the environmental cost of AI. "
  + (Bordage, F., de Montenay, L., Benqassem, S., Delmas-Orgelet, J., Domon, F., Prunel, D., Vateau, C. et Lees Perasso, E. Digital technologies in europe: an environmental life cycle approach.)
*** SOTA :
+ Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and Policy Considerations for Deep Learning in NLP. arXiv:1906.02243 [cs], June 2019. arXiv: 1906.02243
  + "Interest in the energy consumption of AI started in 2019 with the work of Strubell et al."
+ Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of BLOOM, a 176b parameter language model. Journal of Machine Learning Research, 24(253):1–15, 2023.
 + "However, they solely focus on training. A more recent study of the BLOOM model [4] reports the energy consumed by the training, an estimation of the global cost of inference, and an estimation of the manufacturing cost based on LCA."
*** Observations :
+ "AI as a service"
  #+begin_center
  [[file:images/AIaaS.png]]
  #+end_center
+ "The impact is observed through 3 criteria, selected for their availability, quality, and relevance, considering the main impact categories known for digital services [2]."
 + Abiotic Depletion Potential (ADP) for minerals and metals :" It represents the decrease in available resources that have limited reserves."
 + Global Warming Potential (GWP) : evaluates the contribution to climate change.
 + Primary Energy (PE): expresses the total energy footprint.
**** TODO :: Regarding the impacts' formula defines on this paper, depending on the deployments contexts that we want to study, which part of the life cycle of the AI system can we consider ?
+ for a large scale computing deployment context it will be the quiet the same approach observed in this study.
+ Now, regarding an edge computing deployment :
  + How can we consider the network impact?
 + Do we need to consider the web hosting cost?
 + How can we introduce the notion of memory usage for the storage of the model if the inference process is done locally?
+ Nowday, a lot of models use Reinforcement Learining based on Human Feedback (RLHF) to improve the model with the feedback of the user. Can we incorporate that in our study?
*** Results :
#+begin_center
[[file:images/Results_StableD.png]]
#+end_center
**** TODO :: The paper says that they used the Sirius cluster from Grid5000 for the "part of" training and inference process.
+ Did the study reproduces exactly the computation context of FU1 and FU2? If not, what is the appriximation done to observe the results table 2 and figure 2?
+ How did the study to take into account the amount of energy needed to perform the cooling process? Does the hardware power meter used take into account this measure?
** 3° paper : Estimating the carbon footprint of BLOOM, a 176B parameter language model (https://arxiv.org/pdf/2211.02001)
*** Keywords :
Large language models
*** SOTA :
+ STRUBELL, E., GANESH, A., AND MCCALLUM, A. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243 (2019).
 + "Starting with the seminal work of Strubell et al., who looked at the carbon footprint of training a Transformer model"
*** Observations :
+ BigScience Large Open-science Open-access Multilingual Language Model (BLOOM)
+ Model architechture and trainning process :
  #+begin_center
  [[file:images/Bloom_param.png]]
  #+end_center
+ LCA Methodology
  #+begin_center
[[file:images/Bloom_LCA.png]]
#+end_center
+ "For instance, methane has a 100-year GWP 25 times that of CO2– this means that it is equal to 25 CO2eq. "
+ Data used in the paper : https://github.com/bigscience-workshop/carbon-footprint/
+ embodied emissions : "embodied emissions are those emissions associated with the materials and processes involved in producing a given product, such as the computing equipment needed to train and deploy ML models."
+ "While Nvidia does not currently disclose the carbon footprint of its GPUs, recent estimates put the lower bound of this amount at approximately 150 kg of CO2eq [9], which is the number we will use for our embodied emissions estimates."
 + DAVY, B. Building an aws ec2 carbon emission's dataset. https://medium.com/teads-engineering/ building-an-aws-ec2-carbon-emissions-dataset-3f0fd76c98ac, 2021.
+ Embodied emissions "11.2 tonnes of CO2eq to its carbon footprint"
+ thermal design power (TDP) :"le TDP est la quantité maximale de chaleur générée par le composant quand il est utilisé au maximum de sa puissance."
+ Dynamic power consumption : "24.69 tonnes of CO2eq"
+ idle power consumption : "However, it is important to keep in mind that the broader infrastructure that maintains and connects this hardware also requires large amounts of energy to power it – this is referred to as idle consumption."
 + "This can be reflected in part by factoring in the PUE (Power Usage Effectiveness) of the data centers used for training these models, which is the approach adopted by Patterson et al. for estimating the carbon emissions of ML models such as T5 and GPT-3 [28]."(PATTERSON, D., GONZALEZ, J., LE, Q., LIANG, C., MUNGUIA, L.-M., ROTHCHILD, D., SO, D., TEXIER, M., AND DEAN, J. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021))
  + 14.6 tonnes of CO2eq
**** Results :
#+begin_center
[[file:images/Bloom_Results.png]]
#+end_center
+ Comparison with other LLMs :
  #+begin_center
  [[file:images/LLM_Comp.png]]
  #+end_center
  + "A few recent LLM papers reported the carbon footprint of model training, including notable models such as OPT175B [37], GPT-3 [28] and Gopher [29]."
** 4° paper : Green My LLM: Studying the key factors affecting the energy consumption of code assistants (https://arxiv.org/pdf/2411.11892)
*** Keywords :
Large language models (LLM), Integrated Development Environments (IDE), GitHub Copilot, code assistants, power consumption.
*** Citations :
+ "Code assistants offer auto-completion suggestions that developers can either accept or reject."
*** SOTA :
+ R. Schwartz, J. Dodge, N. A. Smith, O. Etzioni, Green AI, Communications of the ACM 63 (12) (2020) 54–63. doi:10.1145/3381831.
 + "Green AI has been defined by Schwartz et al. [1] as research that yields novel results while considering the computational cost, making practitioners ideally reduce resources spent."
+ S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones, W. Bergeron, J. Kepner, D. Tiwari, V. Gadepally, From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference, in: 2023 IEEE High Performance Extreme Computing Conference (HPEC), 2023, pp. 1–9. doi:10.1109/HPEC58863.2023.10363447.
  + "For instance, Samsi et al. benchmarked the energy consumption of LLM inference and were able to estimate the energy of a single response from an LLM [2]."
*** Observations :
+ Why GitHub Copilot ?
  + "it is the most used code assistant according to the 2023 StackOverflow developer survey"
  + "We are aware of the existence of these mechanisms thanks to the Copilot Explorer project"
+ "We designed an experiment with participants using GitHub Copilot to gather a dataset of development traces, enabling us to simulate developers using a code assistant on an inference server under our control. "
+ "All the participants were proficient in Java programming."
+ "GitHub Copilot’s telemetry includes a plethora of different messages that enable us to retrace the history of a generation, from the moment GitHub Copilot decides to send a generation request to the moment of its acceptance from the user."
+ "The dataset is available at https://doi.org/10.5281/zenodo.11503612."
+ "All the artifacts of this study, including our results, code, and datasets, are available in the following public repository: https://doi.org/10.5281/ zenodo.13167546."
+ https://github.com/huggingface/text-generation-inference
+ Studied factors :
  + Number of concurrent developers : 1, 2, 5, 10, 20, 30, 50, 75, 100, 150, 200, 300, 400, 500.
  + Streaming the requests
  + Manually triggering the code assistant
  + Code assistant model : StarCoder (15.5B parameters) [8], StarCoder2-7b and StarCoder2-15b [9] (decoder-only transformers)
  + Quantization method : We studied multiple quantization methods for running the models on the inference server: EETQ [10], BitsAndBytes-NF4, BitsAndBytes-FP4 [11] and no quantization at all.
  + Maximum number of concurrent requests
  + number of GPUs
+" In total, we performed 829 simulations with 314 unique configurations."
+ "We measured the energy consumption of the inference server’s CPU and GPU using perf and nvidia-smi utilities, respectively."
+ "To estimate the carbon emissions related to the energy consumption measured during our experiments, we considered France’s 2023 carbon intensity, equivalent to 56 g of CO2 per kWh [12]"
**** Hardware used :
One server run on :
+ 1 AMD EPYC 7513 (CPU) (https://www.amd.com/fr/products/processors/server/epyc/7003-series/amd-epyc-7513.html) TDP = 200W
+ 4 Nvidia A100-SXM4-40GB (GPUs) (https://www.techpowerup.com/gpu-specs/a100-sxm4-40-gb.c3506) TDP = 800W (Jetson ~ 30W)
*** Results :
+ Energy consumption :
  #+begin_center
  [[file:images/Copilot_energy.png]]
  #+end_center
  + Using LLMs with fewer parameters reduces energy consumption and latency :
    + "For instance, switching from StarCoder2-15B to StarCoder2-7B reduces energy consumption by 15.6% and latency by 10.0%."
  + Increasing the number of GPU increase the energy consumption (2 → 4 ~ 51.8%)
    + "Downsizing the number of GPUs can be a viable option for saving energy if the number of concurrent developers is low enough."
+ Latency :
  #+begin_center
  [[file:images/Copilot_latency.png]]
  #+end_center
  + Using more GPUs can decrease the latency
  + The maximum number of concurrent requests affect a lot the latency (128 → 1000 ~ 597%)
  + Quantization increase latency
  + The model architecture play an important role in terms of latency :
    +" For example, while StarCoder (15.5B) and StarCoder2-15B exhibit a similar number of parameters and power consumption, using the latter over the former increases the latency by 149% on average."
+ Cross picture :
  #+begin_center
  [[file:images/Copilot_cross_pic.png]]
  #+end_center
  + energy consumption per devlopper decrease when we add more users
  + latency increase a lot when adsding more users
**** How much does a developer using a code assistant such as GitHub Copilot cost in energy?
3 diferents scenarios :
+ Small team: 5 developers concurrently requesting a single dedicated server machine.
+ Medium team: 20 developers concurrently requesting a single dedicated server machine.
+ Distributed service (e.g., GitHub Copilot): Many developers (sharing) requesting many server machines. The aim is to maximize the number of developers per machine while not saturating the servers. We report on the impact of a single machine in this scenario.
2 differents objective (for the model defined by the parameter choosen for the inference server(s)):
+ Performance : minimize the latency
+ Frugality : minimize the energy
#+begin_center
[[file:images/Copilot_obj_scenario.png]]
#+end_center
+ We can observe differents results for each scenario and objective:
****** Objective observations :
As we can imagine :
+ The global and the per-user amount of energy needed is higher for the performance objective
+ The average latency is always higher for the frugal objective
****** Scenario observations :
Its more tricky here :
+ If we look at the amount of energy needed by user, we can observe : Small > Medium > Distributed
+ But if we consider the global energy used by the inference servers in each scenario we can see : Distributed > Medium > Small
*** TODO :: Have a deep understanding on quantization
** 5° paper (survey) : Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence (https://arxiv.org/pdf/1909.00560)
*** Authors :
Shuiguang Deng, Hailiang Zhao, Weijia Fang, Jianwei Yin, Schahram Dustdar and Albert Y. Zomaya
*** Keywords :
Edge computing, AI for edge / AI on edge, Edge Intelligence
*** Citations :
+ "Offloading such huge data from the edge to cloud is intractable because it can lead to excessive network congestion. Therefore, a more applicable way is to handle user demands at the edge directly, which leads to the birth of a brand-new computation paradigm, (Mobile → Multi-access) Edge Computing."
+ "The principle of Edge Computing is to push the computation and communication resources from cloud to edge of networks to provide services and perform computations, avoiding unnecessary communication latency and enabling faster responses for end users. "
+ "Application-specific accelerators are designed for further improvement in throughput and energèy efficiency"
+ AI for edge : is a research direction focusing on providing a better solution to constrained optimization problems in Edge Computing with the help of effective AI technologies. Here, AI is used to endow edge with more intelligence and optimality. Therefore, it can be understood as Intelligence-enabled Edge Computing (IEC).
+ AI on edge : studies how to run AI models on edge. It is a framework for running training and inference of AI models with device-edge-cloud synergy, which aims at extracting insights from massive and distributed edge data with the satisfaction of algorithm performance, cost, privacy, reliability, efficiency, etc. Therefore, it can be interpreted as Artificial Intelligence on Edge (AIE).
+ "Edge Computing faces resource allocation problems in different layers, such as CPU cycle frequency, access jurisdiction, radio-frequency, bandwidth, and so on"
+ "Nowadays, it is gradually becoming possible that AI chips with computational acceleration such as Field Programmable Gate Arrays (FPGAs), Graphics Processing Units (GPUs), Tensor Processing Units (TPUs) and Neural Processing Units (NPUs) are integrated with intelligent mobile devices."
+ Edge Inteligence road-map :
  #+begin_center
  [[file:images/EI_Road_map.png]]
  #+end_center
  + "In this paper, we define an edge site as a microdata center with applications deployed, attached to a Small-cell Base Station (SBS). "
+ Quality of Experience (QoE) : Performance, Cost, Privacy (Security), Efficiency and Reliability.
+ "By top-down decomposition, we divide the research efforts in AI on edge into three categories: Model Adaptation, Framework Design and Processor Acceleration."
**** Framework Design :
Framework design aims at providing a better training and inference architecture for the
edge without modifying the existing AI models.
***** Model Trainning :
****** Distributed training framework :
Data splitting :
+ master-device splitting vs helper-device splitting : "The differences lie where the training samples come from and how the global model is assembled and aggregated. "
Model splitting :
+ separates neural networks’ layers and deploys them on different devices. It highly relies on sophisticated pipelines.
****** IDEA :: Knowledge distillation-based frameworks :
+ transfer learning technologies
+ "Knowledge distillation can enhance the accuracy of shallow student networks"
+ Train a basic network → Transfer the learned knowledge to the shallow student model
****** Federated Learning :
+ "proposed to preserve privacy when training the DNNs in a distributed manner"
+ "Federated Learning trains a series of local models on multiple clients."
+ "After that, a global model is optimized by averaging the trained gradients of each client."
+ "In this process, trade-offs between model performance and communication overhead has to be considered."
***** IDEA :: Model Inference :
+ "Model splitting/partitioning can be viewed as a framework for model inference."
+ "A typical example on model inference on edge is [32], where a DNN is split into two parts and carried out collaboratively. The computation-intensive part is running on the edge server while the other is running on the mobile device. "
**** Model Adaptation :
Model Adaptation makes appropriate improvements based on existing training and inference
frameworks to make them more applicable to the edge.
***** Federated learning
***** Model Compression :
Model compression exploits the inherent sparsity structure of gradients and weights to reduce the memory and channel occupation as much as possible.
+ Quantization : such as Binnarized Neural Networks (BNNs): binary weights and activations to replace regular DNNs. This is a typical exploration of quantization.
+ Dimension reduction : (Examples for federated learning)
  + structured updates : the local update is learnt from a restricted lower-dimensional space.
  + sketched updates : the uploading model is compressed before sending to the central server.
+ Prunning : suggests that DNNs are typically over-parameterized and their weights have significant redundancy.  Meanwhile, pruning compensates for the loss in performance.
+ Precision downgrading
+ Component Sharing
+ Cutoff
+ "Model Compression is suitable for both Model Training and Model Inference."
"Those approaches can be realized by methods such as Singular Value Decomposition (SVD), Huffman Coding, Principal Component Analysis (PCA) and several others."
***** Conditional Computation :
Conditional computation is an alternative way to reduce the amount of calculation by selectively turning off some unimportant calculations of DNNs. It can be viewed as block wise dropout.
+ Components Shutoff : It selects the most valuable participators in Federated Learning for model updates.
+ Input Filtering
+ Early exit : early stop if the confident threshold is achieved.
+ Results caching
***** Algorithm Asynchronization :
Aggregating local models in an asynchronous way.y. It is designed for overcoming the inefficient and lengthy synchronous steps of model updates in Federated Learning.
***** Thorough Decentralization :
Removes the central aggregator to avoid any possible leakage and address the central server’s malfunction. The ways to achieve totally decentralization include but not limited to blockchain technologies and game-theoretical approaches
**** Processor acceleration :
Optimizations of the computations required by DNNs at the hardware level :
+ Designing special instructuion set for DNNs
+ Designing highly paralleled computing paradigms
+ Moving computation closer to the memory
*** Problematic : How to carry out the model training and inference on resource-scarce devices ?
Model adaptation methods :
#+begin_center
[[file:images/EI_adapt.png]]
#+end_center
+ "Thakker et al. study a compressed RNN cell implementation called Hybrid Matrix Decomposition (HMD) for model inference [57]."
** Thursday 6 february
*** What has been done :
+ Review of an anlysis over the energy consumption of the inference process for NLP/CV models.
+ Review of the LCA based methodology to estimate the environmental impact of GenAI model (Stable diffussion).
+ Review of a paper which measure the enrgy consumption of the trainning of BLOOM language model.
+ Review of a methodology to evaluate the eneregy consumption/environemental impact of the deployment of a coding assistant (StarCoder) in a controlled environment.
+ Review of a survey about "Edge Intelligence" which prensent diverse method to allow AI system to be deployed on edge devices, this paper also present a pipeline to evaluate the Quality of Experience (QoE) for an "AI on edge" system.
*** Observations :
+ Paper 1 :
  + The study provides us an overview of the energy consumption of the inference process of a huge range of AI systems, that allow us to uderstand the global trend of the phenomenom.
  + The study take in account only one inference process without taking into account the multiplactive factor defined by the usation of an AI system by multiple users
+ Don't take in account the environnemental impact
+ Paper 2 :
  + The paper provide us a relevant way to estimate the environnemental impact of each part of the life cycle of an AI system.
  + This paper provides us also a case study over Stable Diffusion, that highlight the non-negligable cost of the inference process among the global life cycle of the model.
  + The paper use a constant to estimate the impact of the cooling system (Because it's difficult to measure it by using grid5000), But in the case where we use edge devices for the computation of the inference process we can measure the energy consumption of the cooling system, thus the impact of this process. This measurment difference between edge deployment and large scale deployment can introduce a bias if we want to compare our results.
+ Paper 3 :
  + The paper provide us a comparative an analysis of the Co2 emission of the training of multiple LLM
  + But this study is more focused on the trainning phase, and they do approximation for the inference process, so I don't know if we can use it for our study.
+ Paper 4 :
  + The paper provide us a relevant analysis on the impact (latency, energy consumptio) of different parameter regarding the inference deployment of AI system (number of user, number of GPU, model size, model architecture, ...)
  + The paper provide us a well-defined methodology with a strong reproductibility. I think that we can adapt this methodology to an edge deployment context.
  + They used a linear function to measure the CO2 emission of the inference process from its energy consumption.
  + In the results section, they present that, from a given number of developper using the inference server concurently (~100), the power consumption of attain a "roofline", thus the eneregy consumption per user decrease a lot by adding more user. My analysis of this roofline is that the inference server attain it's peak performance (FLOPs/sec) and by the way it's peak energy consumption. But I think that using the server at its peak performance during a long period of time introduce another energy consumption source which must be considered : the energy used to maintain the proceqssors of the server at a stable temperature (cooling process).
  + Finally the study didn't include a measure of the performance for the differents setting such as BLUE.
+ Paper 5 :
  + The survey provide us a pipeline for evaluate the performance of AI models deploy on edge devices based on a Quality of Experience (QoE) approach.
  + The survey provide us a huge ammount of method to reduce the size and decrease the computation's requirement of AI models to handle the constraints of edges devices for their deployments (a low energy consumption and a lack of memory space).
*** What to do next :
**** TODO :: Find technical documentation about edges devices to represent the computational constraints of edge devicies by values
+ Nvidia Jetson AGX Xavier
+ STMicro hardware with low consumption
**** TODO :: Read the documentation about the model what we want to study to define what modification of them must be done to statisfy the computational constraints of edge devices
+ Starcoder
+ Bloom
+ Deepseek
+ Mistral
+ ...
** 6° paper : Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models (https://eprints.iisc.ac.in/80120/1/ori_ACM_mea_6-3_2022.pdf)
*** Authors :
PRASHANTHI S.K, SAI ANUROOP KESANAPALLI, and YOGESH SIMMHAN
*** Keywords :
DNNs, edge computing devices, Nvidia Jetson
*** SOTA :
+ Hazem A. Abdelhafez, Hassan Halawa, Karthik Pattabiraman, and Matei Ripeanu. 2021. Snowflakes at the Edge: A Study of Variability among NVIDIA Jetson AGX Xavier Boards. In ACM EdgeSys Workshop.
  + "Snowflakes [1] reports a detailed study on the latency and power variability observed across Jetson AGX Xaviers for inferencing. "
*** Citations :
+ "NVIDIA Jetson AGX Xavier kit has a 512-core Volta GPU, an 8-core ARM CPU and 32𝐺𝐵 of LPDDR4x memory, that operates within 65𝑊 of power"
 #+begin_center
 [[file:images/Jetson_hard_table.png]]
 #+end_center
+ Nvidia Jetson microarchitecture ~ workstation/server GPU microarchitecture (with fewer cores)
+ "Jetson devices are available as accelerator modules with CPU, GPU and memory"
+ what is TOPS performance measure?
+ "The RAM is shared between the CPU and GPU" → increase the amount of memory available for the GPU → but make slower the communication with the memory.
  + ". Also, the LPDDR RAM used in these devices is slower and low-powered, as opposed to GDDR that is used in regular GPUs."
+ differents power modes → differents cores / CPU frequency / GPU frequency (power/performance trade off)
+ support a wide variety of storage media (eMMC, Micro SD card, NVME Solid State Drive (SSD), Hard Disk Drive (HDD)...) not the same costs/perfs.
*** DNN Trainning :
#+begin_center
[[file:images/Edge_DNN_Training.png]]
#+end_center
+ In each iteration (epoch) :
  + fetch a "mini-batch" from disk memory
  + pre-process the mini batch (CPU)
  + CPU launches kernel on GPU (forward/backward pass)
  + repeat to consume the input sample
*** Method :
+ "The default power mode is the highest rated for all devices: MAXN for the AGX and Nano, and 15𝑊 for NX"
  #+begin_center
  [[file:images/Table_pm.png]]
  #+end_center
+ "The fan speed is set to maximum to avoid resource throttling due to overheating. "
*** Perdformance Metrics :
+ "average and instantaneous power are measured using the jtop Python module, which internally uses the tegrastats [41] utility from NVIDIA, at ≈ 1 𝑠 sampling."
+ "The power measurements are from on-board sensors in the Jetsons, which capture the power load from the module but not the carrier board and peripherals"
+ "However, the bulk of the variation in the energy usage during training is from the module load."
+ "So the total energy for training in a duration 𝑇 is calculated as a sum of the instantaneous power (𝑝𝑡𝑖 in watts) measured at time 𝑡𝑖, weighted by the duration between successive samples (𝑡𝑖 −𝑡𝑖−1), given as:
        sum{𝑡𝑖 ∈𝑇} (𝑝𝑡𝑖 · (𝑡𝑖 −𝑡𝑖−1))"
*** Results :
+  The scripts and logs for these are available at https://github.com/dream-lab/edge-train-bench/tree/sigmetrics-2023
+ Number of Fetch/Pre-process Workers : With 𝑤 > 1, multiple workers perform the fetch and pre-process in parallel to get batches ready for a separate GPU compute process.
  #+begin_center
  [[file:images/Jetson_workers.png]]
  #+end_center
*** Reunion preparation 21/02 14h30
+ synthèse du travail d'analyse
+ question de recherche
  + First idea : Which conditions allow edge computing deployments to be more efficient than large-scale deployments for inference?
  + Efficiency : In our context we can consider efficiency with a QoE approach where the cost refer to the environemental impact + latencty.
+ hypothèses
+ designer la methodologie pour répondre au question de recherche
  + est ce qu'on a tout ce qu'il faut?
** 7° paper : How Green Can AI Be? A Study of Trends in Machine Learning Environmental Impacts (https://arxiv.org/pdf/2412.17376)
*** Authors :
Clement Morand, Anne-Laure Ligozat, Aurelie Neveol
*** Keywords :
AI, NVIDIA, Environemental impact, Rebound effect.
*** Citations :
+ "Specifically, we investigate how the impacts of individual graphics cards have evolved in the past 10 years and how the environmental impacts associated with training ML models have evolved over this period."
*** SOTA :
+ "After the high level of carbon emissions associated with training Natural Language processing models was reported (Strubell, Ganesh, and McCallum 2019),"
+ "A position paper by de Vries (2023) discussed the potential growth of the ML sector with the recent surge in demand for large language models freely accessible as chatbots."
+ "Masanet et al. (2020) has shown that despite an exponential growth in computation demand between 2012 and 2018, the total energy consumption of datacenters had only increased by around 5% indicates that ”A [...] source of higher electricity consumption is coming from energy-intensive data centres, artificial intelligence (AI) and cryptocurrencies, which could double by 2026.”"
*** Observations :
+ "Life cycle assessments of ICT equipment have shown the importance of Integrated Circuits (IC) in the environmental impacts of ICT equipment (Clement, Jacquemotte, and Hilty 2020)."
  + GPU (logic type ICs)
  + memory (memory type ICs)
+ die area = surface of the GPU
+ "Thus, if the die area increases we can expect an increase in the environmental impacts of the device."
+ "The dataset is based on data retrieved from the TechPowerUp GPU database"
+ ”Models that have advanced the state of the art, had a large influence in the field’s history, or had a large impact within the world”
  + https://epochai.org/data/notable-ai-models-documentation
#+begin_center
[[file:images/GreenAI_Nvidia_evo.png]]
#+end_center
+ Die area has increased linearly while memory size has grown exponentially
+ Exponential growth in the memory size does not necessarily mean a proportional increase in memory IC area as Moore’s Law has allowed to exponentially miniaturize ICs.
#+begin_center
[[file:images/GreenAI_Nvidia_nrj.png]]
#+end_center
+ Even if the energy consumption per operation has decreased over time, the total energy consumption of a card has slightly increased over time (rebbound effect)
#+begin_center
[[file:images/GreenAI_Nvidia_manu.png]]
#+end_center
+ Overall, all the important characteristics of graphics cards produced by NVIDIA for workstation have increased, leading to an increase in the environmental impacts incurred by the production of these cards over time.
#+begin_center
[[file:images/GreenAI_ML.png]]
#+end_center
+ Hardware quantity has increased exponentially.(y-logscale)
**** IDEA :: Data on AI : https://epoch.ai/data
** 8° paper : StarCoder: may the source be with you! (https://arxiv.org/pdf/2305.06161)
*** Keywords :
 Large Language Models for Code (Code LLMs)
*** SOTA :
+ Most of these models have not been fully open, but PolyCoder (Xu et al., 2022) and SantaCoder (Ben Allal et al., 2023) are notable exceptions and have both open models and training data.
*** Citations :
+ "GitHub reports that Copilot users rely on it to produce 35% of the code they write for some languages."
+ "StarCoder and StarCoderBase, open-access code LLMs developed and released by the BigCode community, with a focus on respecting copyright, privacy, transparency, and community-driven model development."
+ The Stack : publicly available pre-training dataset for Code LLMs with a transparent data governance framework (6.4 TB, 384 programming language)
+ StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack.
+ We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages
+ We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model.
+ Improvement :
  + 8K token context length
  + fast large-batch inference through Multi-Query-Attention (MQA)
+ "The Transformer architecture led to the development of highly scalable language models which have shown a predictable relationship b etween language modeling loss and scaling factors such as the model size, number of training tokens, and compute budget"
+ "We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task."
*** Observations :
StarEncoder (encoder only model) : Personally Identifiable Information (PII) detection for data pre-processing (i.e., bi-directionally self-attentive Transformers ~ BERT)
**** Model Training :
***** Data formating :
+ Template : <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields.
+ Code/ Script : <reponame>reponame<filename>filename<gh_stars>stars\ncode<|endoftext|>
  + "To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2."
  + code → fill-in-the-middle transformation (FIM): https://arxiv.org/abs/2207.14255
+ Issue (~ GitHub FAQ) : <issue_start>Title:title\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|>
  + idx = an anonymized speaker identifier
+ Jupyter structured : <jupyter_start><jupyter_text>text0<jupyter_code>code0<jupyter_output>output0<jupyter_text> ... <|endoftext|>
+ Git commits : <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|>
***** Tokenizer :
+ Hugging Face Tokenizers library → train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens
  + The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer.
***** Model architechture
+ 15.5B parameters
+ Same architechture as SantaCoder :
  + Decoder-only Transformer
  + Multi-Query-Attention : http://arxiv.org/abs/1911.02150
  + FlashAttention : Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memoryefficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.
    + "to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length."
  + "To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel."
[[file:images/StarsCoder_archi.png]]
****** TODO :: Understanding validating :
+ Hidden size = number of hidden layer in the FFNs
+ Intermediate size = size of query, key, value and embedings vectors
+ Max positional embeddings = maximum number of input tokens
+ Num. Of attention heads = number of heads used during the multi-head attention process
+ Num. Of hidden layers = number of decoder block in the global decoder stack which define the model.
***** Training details :
+ StarCoderBase :
  + 250k iterations
  + Batch size = 4M tokens
  + 1 trillion token in total
  + Adam optimizer : β1 = 0.9, β2 = 0.95, ϵ = 10−8
  + Learning rate : cosine decay from 3 × 10−4 to 3 × 10−5 after a linear warmup of 2,000 iterations
+ StarCoder : (finetunning)
  + Python variant of the model
  + 2 epochs
  + We used the same settings as StarCoderBase, except that we used a learning rate of 5 × 10−5 and decayed it to 5 × 10−6 after 1,000 iterations of linear warmup.
  + 8500 iterations
***** TODO :: Hardware used : (to better understand)
+ GPU clusters :
  + 512 A100 80 GB GPUs
  + Distributed across 64 nodes
  + 3D parallel layout partitioning : "that shards the model with both tensor and pipeline parallelism rank 4"
  + 16 GPUs (two nodes) for one replica
  + 32-fold data parallelism
+ Megatron-LM's distributed optimizer : it leads to slightly higher throughput in this configuration.
  + "Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities."(BF16=brain floating point)
+ Total number of GPU hours for training : 320,256
+ 280W per GPU → 89671.68 kWh of electricity consumed during the trainning process
+ carbon intensity of the energy of the us-west-2 AWS location : 0.15495 kgCO2e per kWh
+ PUE AWS = 1.2
+ 16.68 tonnes of CO2eq emitted
  + finetunning : 3.5% of the training time  0.58 tonnes of CO2eq.
***** Evaluation :
+ Code LM Evaluation Harness : This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution.
+ HumanEval/MBPP : widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by a Code LLM
+ Code LLMs generate code by sampling from their output distribution.
+ pass@k : the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case.
[[file:images/StarCoder_eval_1.png]]
+  DS-1000 benchmark : a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases.
  + 2 eval modes : completion vs insertion
[[file:images/StarCoder_eval_2.png]]
+ "model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks."
+ ODEX benchmark : 505 open-domain and 440 closed-domain Python coding queries, in four natural languages — English, Spanish, Japanese, and Russian — with test-case-based execution evaluation.
  + closed-domain : using only built-in Python functions
  + open-domain : using functions from imported libraries
[[file:images/StarCoder_eval_3.png]]
** 9° paper : MLCA: a tool for Machine Learning Life Cycle Assessment (https://hal.science/hal-04643414v1/file/ICT4S_Paper.pdf)
*** Date : 10 Jul 2024
*** Authors :
Clement Morand, Anne-Laure Ligozat, Aurelie Neveol
*** Keywords :
Graphics card manufacturacturing impacts, LCA,
*** SOTA :
*** Citations :
+ " The impacts of producing the hardware are not addressed in spite of their significance."
+ "LCA does not account for structural or societal impacts such as rebound effect"
*** Observations :
+ Machine Learning life Cycle Assessment (MLCA) : aimed at providing researchers with LCA estimates of computation impact that can be obtained independently from running calculations.
+ The carbon footprint is computed by all tools as follows:
#+BEGIN_EXPORT latex
\begin{equation}
GWP = CI ∗ PUE ∗ (p_c + p_g + p_m) ∗ t
\end{equation}
#+END_EXPORT
