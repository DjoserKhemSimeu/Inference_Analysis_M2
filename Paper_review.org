* Paper notes : Estimating the environmental impacts of AI inference deployments
*** Djoser SIMEU M2 MOSIG DSAI / DATAMOVE
** 1° paper : Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning (https://www.sciencedirect.com/science/article/pii/S2210537923000124?ref=pdf_download&fr=RR-2&rr=90c1945e6bdfbb5c)
*** Authors : Radosvet Desislavov, Fernando Martínez-Plumed, José Hernández-Orallo
*** Keywords :
inference costs, energy consumption, computer vision, natural language processing, carbon footprint, FLOPs,
*** Citations:
+ "At first look it seems that training cost is higher. However, for deployed systems, inference costs exceed training costs, because of the multiplicative factor of using the system many times."
+ "We focus our analysis on inference FLOPs (Floating Point Operations) required to process one input item (image or text fragment)."
*** SOTA :
+ "AI efficiency" : D. Hernandez, T.B. Brown, Measuring the algorithmic efficiency of neural networks, 2020, arXiv:2005.04305.
" This study shows that 44 times less compute
was required in 2020 to train a network with the same performancevAlexNet achieved seven years before."
+ "D. Amodei, D. Hernandez, Ai and compute, 2018, https://openai.com/blog/aiand-compute/."
 "As a result, it has been estimated that AI models have doubled the computational power they use every 3.4 months since 2012" + "For instance, new algorithms and architectures such as EfficientNet [26] and EfficientNetV2 [27] have aimed at this reduction in compute"
*** Observations :
+ The number of parameters and FLOPs: "Note that recent transformer models [45] do not follow the growth relation presented above. However, the correlation between the number of parameters and FLOPs for CNNs is 0.772 and the correlation for transformers is 0.994 (Fig. 1)."
***** TODO :: How do they compute FLOPs for transformer models
+ "From the FLOPS and power consumption we calculate the efficiency, dividing FLOPS by Watts."
 #+begin_center
  [[file:images/Energy_Efficiency_comp.png]]
  #+end_center
+ "For NLP, the trends are similar, but the best models are growing much faster, as we see in Fig. 14, while the regular models may even decrease. "
  #+begin_center
[[file:images/NLP_Task_Joules.png]]
[[file:images/NLP_Task_GLUE.png]]
#+end_center
** 2° paper : Estimating the environmental impact of Generative-AI services using an LCA-based methodology (https://www.sciencedirect.com/science/article/pii/S2212827124001173)
*** Authors : Adrien Berthelot, Eddy Caron, Mathilde Jay, Laurent Lefevre
*** Keywords :
 LCA-based methodology, environmental impact of generative AI services, Stable Diffusion
*** Citations :
+ "In the context of environmental challenges and considering the footprint of the digital sector, which in the EU already accounts for 9.3% of electricity consumption and over 4% of greenhouse gas emissions, many studies have addressed the question of the environmental cost of AI. "
  + (Bordage, F., de Montenay, L., Benqassem, S., Delmas-Orgelet, J., Domon, F., Prunel, D., Vateau, C. et Lees Perasso, E. Digital technologies in europe: an environmental life cycle approach.)
*** SOTA :
+ Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and Policy Considerations for Deep Learning in NLP. arXiv:1906.02243 [cs], June 2019. arXiv: 1906.02243
  + "Interest in the energy consumption of AI started in 2019 with the work of Strubell et al."
+ Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of BLOOM, a 176b parameter language model. Journal of Machine Learning Research, 24(253):1–15, 2023.
 + "However, they solely focus on training. A more recent study of the BLOOM model [4] reports the energy consumed by the training, an estimation of the global cost of inference, and an estimation of the manufacturing cost based on LCA."
*** Observations :
+ "AI as a service"
  #+begin_center
  [[file:images/AIaaS.png]]
  #+end_center
+ "The impact is observed through 3 criteria, selected for their availability, quality, and relevance, considering the main impact categories known for digital services [2]."
 + Abiotic Depletion Potential (ADP) for minerals and metals :" It represents the decrease in available resources that have limited reserves."
 + Global Warming Potential (GWP) : evaluates the contribution to climate change.
 + Primary Energy (PE): expresses the total energy footprint.
**** TODO :: Regarding the impacts' formula defines on this paper, depending on the deployments contexts that we want to study, which part of the life cycle of the AI system can we consider ?
+ for a large scale computing deployment context it will be the quiet the same approach observed in this study.
+ Now, regarding an edge computing deployment :
  + How can we consider the network impact?
 + Do we need to consider the web hosting cost?
 + How can we introduce the notion of memory usage for the storage of the model if the inference process is done locally?
+ Nowday, a lot of models use Reinforcement Learining based on Human Feedback (RLHF) to improve the model with the feedback of the user. Can we incorporate that in our study?
*** Results :
#+begin_center
[[file:images/Results_StableD.png]]
#+end_center
**** TODO :: The paper says that they used the Sirius cluster from Grid5000 for the "part of" training and inference process.
+ Did the study reproduces exactly the computation context of FU1 and FU2? If not, what is the appriximation done to observe the results table 2 and figure 2?
+ How did the study to take into account the amount of energy needed to perform the cooling process? Does the hardware power meter used take into account this measure?
** 3° paper : Estimating the carbon footprint of BLOOM, a 176B parameter language model (https://arxiv.org/pdf/2211.02001)
*** Keywords :
Large language models
*** SOTA :
+ STRUBELL, E., GANESH, A., AND MCCALLUM, A. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243 (2019).
 + "Starting with the seminal work of Strubell et al., who looked at the carbon footprint of training a Transformer model"
*** Observations :
+ BigScience Large Open-science Open-access Multilingual Language Model (BLOOM)
+ Model architechture and trainning process :
  #+begin_center
  [[file:images/Bloom_param.png]]
  #+end_center
+ LCA Methodology
  #+begin_center
[[file:images/Bloom_LCA.png]]
#+end_center
+ "For instance, methane has a 100-year GWP 25 times that of CO2– this means that it is equal to 25 CO2eq. "
+ Data used in the paper : https://github.com/bigscience-workshop/carbon-footprint/
+ embodied emissions : "embodied emissions are those emissions associated with the materials and processes involved in producing a given product, such as the computing equipment needed to train and deploy ML models."
+ "While Nvidia does not currently disclose the carbon footprint of its GPUs, recent estimates put the lower bound of this amount at approximately 150 kg of CO2eq [9], which is the number we will use for our embodied emissions estimates."
 + DAVY, B. Building an aws ec2 carbon emission's dataset. https://medium.com/teads-engineering/ building-an-aws-ec2-carbon-emissions-dataset-3f0fd76c98ac, 2021.
+ Embodied emissions "11.2 tonnes of CO2eq to its carbon footprint"
+ thermal design power (TDP) :"le TDP est la quantité maximale de chaleur générée par le composant quand il est utilisé au maximum de sa puissance."
+ Dynamic power consumption : "24.69 tonnes of CO2eq"
+ idle power consumption : "However, it is important to keep in mind that the broader infrastructure that maintains and connects this hardware also requires large amounts of energy to power it – this is referred to as idle consumption."
 + "This can be reflected in part by factoring in the PUE (Power Usage Effectiveness) of the data centers used for training these models, which is the approach adopted by Patterson et al. for estimating the carbon emissions of ML models such as T5 and GPT-3 [28]."(PATTERSON, D., GONZALEZ, J., LE, Q., LIANG, C., MUNGUIA, L.-M., ROTHCHILD, D., SO, D., TEXIER, M., AND DEAN, J. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021))
  + 14.6 tonnes of CO2eq
**** Results :
#+begin_center
[[file:images/Bloom_Results.png]]
#+end_center
+ Comparison with other LLMs :
  #+begin_center
  [[file:images/LLM_Comp.png]]
  #+end_center
  + "A few recent LLM papers reported the carbon footprint of model training, including notable models such as OPT175B [37], GPT-3 [28] and Gopher [29]."
** 4° paper : Green My LLM: Studying the key factors affecting the energy consumption of code assistants (https://arxiv.org/pdf/2411.11892)
*** Keywords :
Large language models (LLM), Integrated Development Environments (IDE), GitHub Copilot, code assistants, power consumption.
*** Citations :
+ "Code assistants offer auto-completion suggestions that developers can either accept or reject."
*** SOTA :
+ R. Schwartz, J. Dodge, N. A. Smith, O. Etzioni, Green AI, Communications of the ACM 63 (12) (2020) 54–63. Doi:10.1145/3381831.
 + "Green AI has been defined by Schwartz et al. [1] as research that yields novel results while considering the computational cost, making practitioners ideally reduce resources spent."
+ S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones, W. Bergeron, J. Kepner, D. Tiwari, V. Gadepally, From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference, in: 2023 IEEE High Performance Extreme Computing Conference (HPEC), 2023, pp. 1–9. doi:10.1109/HPEC58863.2023.10363447.
  + "For instance, Samsi et al. benchmarked the energy consumption of LLM inference and were able to estimate the energy of a single response from an LLM [2]."
*** Observations :
+ Why GitHub Copilot ?
  + "it is the most used code assistant according to the 2023 StackOverflow developer survey"
  + "We are aware of the existence of these mechanisms thanks to the Copilot Explorer project"
+ "We designed an experiment with participants using GitHub Copilot to gather a dataset of development traces, enabling us to simulate developers using a code assistant on an inference server under our control. "
+ "All the participants were proficient in Java programming."
+ "GitHub Copilot’s telemetry includes a plethora of different messages that enable us to retrace the history of a generation, from the moment GitHub Copilot decides to send a generation request to the moment of its acceptance from the user."
+ "The dataset is available at https://doi.org/10.5281/zenodo.11503612."
+ "All the artifacts of this study, including our results, code, and datasets, are available in the following public repository: https://doi.org/10.5281/ zenodo.13167546."
+ https://github.com/huggingface/text-generation-inference
+ Studied factors :
  + Number of concurrent developers : 1, 2, 5, 10, 20, 30, 50, 75, 100, 150, 200, 300, 400, 500.
  + Streaming the requests
  + Manually triggering the code assistant
  + Code assistant model : StarCoder (15.5B parameters) [8], StarCoder2-7b and StarCoder2-15b [9] (decoder-only transformers)
  + Quantization method : We studied multiple quantization methods for running the models on the inference server: EETQ [10], BitsAndBytes-NF4, BitsAndBytes-FP4 [11] and no quantization at all.
  + Maximum number of concurrent requests
  + number of GPUs
+" In total, we performed 829 simulations with 314 unique configurations."
+ "We measured the energy consumption of the inference server’s CPU and GPU using perf and nvidia-smi utilities, respectively."
+ "To estimate the carbon emissions related to the energy consumption measured during our experiments, we considered France’s 2023 carbon intensity, equivalent to 56 g of CO2 per kWh [12]"
**** Hardware used :
One server run on :
+ 1 AMD EPYC 7513 (CPU) (https://www.amd.com/fr/products/processors/server/epyc/7003-series/amd-epyc-7513.html) TDP = 200W
+ 4 Nvidia A100-SXM4-40GB (GPUs) (https://www.techpowerup.com/gpu-specs/a100-sxm4-40-gb.c3506) TDP = 800W (Jetson ~ 30W)
*** Results :
+ Energy consumption :
  #+begin_center
  [[file:images/Copilot_energy.png]]
  #+end_center
  + Using LLMs with fewer parameters reduces energy consumption and latency :
    + "For instance, switching from StarCoder2-15B to StarCoder2-7B reduces energy consumption by 15.6% and latency by 10.0%."
  + Increasing the number of GPU increase the energy consumption (2 → 4 ~ 51.8%)
    + "Downsizing the number of GPUs can be a viable option for saving energy if the number of concurrent developers is low enough."
+ Latency :
  #+begin_center
  [[file:images/Copilot_latency.png]]
  #+end_center
  + Using more GPUs can decrease the latency
  + The maximum number of concurrent requests affect a lot the latency (128 → 1000 ~ 597%)
  + Quantization increase latency
  + The model architecture play an important role in terms of latency :
    +" For example, while StarCoder (15.5B) and StarCoder2-15B exhibit a similar number of parameters and power consumption, using the latter over the former increases the latency by 149% on average."
+ Cross picture :
  #+begin_center
  [[file:images/Copilot_cross_pic.png]]
  #+end_center
  + energy consumption per devlopper decrease when we add more users
  + latency increase a lot when adsding more users
**** How much does a developer using a code assistant such as GitHub Copilot cost in energy?
3 diferents scenarios :
+ Small team: 5 developers concurrently requesting a single dedicated server machine.
+ Medium team: 20 developers concurrently requesting a single dedicated server machine.
+ Distributed service (e.g., GitHub Copilot): Many developers (sharing) requesting many server machines. The aim is to maximize the number of developers per machine while not saturating the servers. We report on the impact of a single machine in this scenario.
2 differents objective (for the model defined by the parameter choosen for the inference server(s)):
+ Performance : minimize the latency
+ Frugality : minimize the energy
#+begin_center
[[file:images/Copilot_obj_scenario.png]]
#+end_center
+ We can observe differents results for each scenario and objective:
****** Objective observations :
As we can imagine :
+ The global and the per-user amount of energy needed is higher for the performance objective
+ The average latency is always higher for the frugal objective
****** Scenario observations :
Its more tricky here :
+ If we look at the amount of energy needed by user, we can observe : Small > Medium > Distributed
+ But if we consider the global energy used by the inference servers in each scenario we can see : Distributed > Medium > Small
*** TODO :: Have a deep understanding on quantization
** 5° paper (survey) : Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence (https://arxiv.org/pdf/1909.00560)
*** Keywords :
Edge computing, AI for edge / AI on edge, Edge Intelligence
*** Citations :
+ "Offloading such huge data from the edge to cloud is intractable because it can lead to excessive network congestion. Therefore, a more applicable way is to handle user demands at the edge directly, which leads to the birth of a brand-new computation paradigm, (Mobile → Multi-access) Edge Computing."
+ "The principle of Edge Computing is to push the computation and communication resources from cloud to edge of networks to provide services and perform computations, avoiding unnecessary communication latency and enabling faster responses for end users. "
+ " Application-specific accelerators are designed for further improvement in throughput and energèy efficiency"
+ AI for edge : is a research direction focusing on providing a better solution to constrained optimization problems in Edge Computing with the help of effective AI technologies. Here, AI is used to endow edge with more intelligence and optimality. Therefore, it can be understood as Intelligence-enabled Edge Computing (IEC).
+ AI on edge : studies how to run AI models on edge. It is a framework for running training and inference of AI models with device-edge-cloud synergy, which aims at extracting insights from massive and distributed edge data with the satisfaction of algorithm performance, cost, privacy, reliability, efficiency, etc. Therefore, it can be interpreted as Artificial Intelligence on Edge (AIE).
+ "Edge Computing faces resource allocation problems in different layers, such as CPU cycle frequency, access jurisdiction, radio-frequency, bandwidth, and so on"
+ "Nowadays, it is gradually becoming possible that AI chips with computational acceleration such as Field Programmable Gate Arrays (FPGAs), Graphics Processing Units (GPUs), Tensor Processing Units (TPUs) and Neural Processing Units (NPUs) are integrated with intelligent mobile devices."
+ Edge Inteligence road-map :
  #+begin_center
  [[file:images/EI_Road_map.png]]
  #+end_center
  + "In this paper, we define an edge site as a microdata center with applications deployed, attached to a Small-cell Base Station (SBS). "
+ Quality of Experience (QoE) : Performance, Cost, Privacy (Security), Efficiency and Reliability.
+ "By top-down decomposition, we divide the research efforts in AI on edge into three categories: Model Adaptation, Framework Design and Processor Acceleration."
**** Framework Design :
Framework design aims at providing a better training and inference architecture for the
edge without modifying the existing AI models.
***** Model Trainning :
****** Distributed training framework :
Data splitting :
+ master-device splitting vs helper-device splitting : "The differences lie where the training samples come from and how the global model is assembled and aggregated. "
Model splitting :
+ separates neural networks’ layers and deploys them on different devices. It highly relies on sophisticated pipelines.
****** IDEA :: Knowledge distillation-based frameworks :
+ transfer learning technologies
+ "Knowledge distillation can enhance the accuracy of shallow student networks"
+ Train a basic network → Transfer the learned knowledge to the shallow student model
****** Federated Learning :
+ "proposed to preserve privacy when training the DNNs in a distributed manner"
+ "Federated Learning trains a series of local models on multiple clients."
+ "After that, a global model is optimized by averaging the trained gradients of each client."
+ "In this process, trade-offs between model performance and communication overhead has to be considered."
***** IDEA :: Model Inference :
+" Model splitting/partitioning can be viewed as a framework for model inference."
+ "A typical example on model inference on edge is [32], where a DNN is split into two parts and carried out collaboratively. The computation-intensive part is running on the edge server while the other is running on the mobile device. "
**** Model Adaptation :
Model Adaptation makes appropriate improvements based on existing training and inference
frameworks to make them more applicable to the edge.
***** Federated learning
***** TODO :: Model Compression :
Model compression exploits the inherent sparsity structure of gradients and weights.
+ Quantization :
+ Dimension reduction :
+ Prunning :
+ Precision downgrading :
+ Component Sharing :
+ Cutoff :
+ ...
"Those approaches can be realized by methods such as Singular Value Decomposition (SVD), Huffman Coding, Principal Component Analysis (PCA) and several others."
***** TODO :: Conditional Computation :
Conditional computation is an alternative way to reduce the amount of calculation by selectively turning off some unimportant calculations of DNNs. It can be viewed as block wise dropout.
+ Components Shutoff :
+ Input Filtering :
+ Early exit :
+ Results caching :
+ ...
***** Algorithm Asynchronization :
Aggregating local models in an asynchronous way.y. It is designed for overcoming the inefficient and lengthy synchronous steps of model updates in Federated Learning.
***** Thorough Decentralization :
Removes the central aggregator to avoid any possible leakage and address the central server’s malfunction. The ways to achieve totally decentralization include but not limited to blockchain technologies and game-theoretical approaches
**** Processor acceleration :
Optimizations of the computations required by DNNs at the hardware level :
+ Designing special instructuion set for DNNs
+ Designing highly paralleled computing paradigms
+ Moving computation closer to the memory
*** Problematic : How to carry out the model training and inference on resource-scarce devices ?
Model adaptation methods :
#+begin_center
[[file:images/EI_adapt.png]]
#+end_center
+" Model Compression is suitable for both Model Training and Model Inference."
+ Binnarized Neural Networks (BNNs): binary weights and activations to replace regular DNNs. This is a typical exploration of quantization.
+ "Thakker et al. study a compressed RNN cell implementation called Hybrid Matrix Decomposition (HMD) for model inference [57]."
